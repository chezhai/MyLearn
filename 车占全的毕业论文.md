1.  损失函数

    1.  选取损失函数

损失函数选取主要参考两个方面：损失函数给出的是单个样本的损失，损失函数是损失函数在所有样本上的和。

现代的神经网络大多数采用最大似然准则。因此损失函数为：

|   | J\\left( \\overrightarrow{\\theta} \\right) = - \\mathbb{E}_{\\overrightarrow{x},y\\sim{\\hat{p}}_{\\text{data}}}\\log p_{\\text{model}}(y\|(\\overrightarrow{x};\\overrightarrow{\\theta}) |   |
|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|


其中$${\hat{p}}_{\text{data}}$$为样本的经验分布，服从：

|   | {\\hat{p}}_{\\text{data}}\\left( {\\overrightarrow{x}}_{y},y_{i} \\right) = \\left\\{ \\begin{matrix}                                                                                                                                                                                             |   |
|   | \\frac{1}{N}\\sigma\\left( \\overrightarrow{x} - {\\overrightarrow{x}}_{i},y - y_{i} \\right),({\\overrightarrow{x}}_{i},y_{i}\\mathbb{) \\in D} \\\\                                                                                                                                             |   |
|   | 0,\\ else \\\\                                                                                                                                                                                                                                                                                    |   |
|   | \\end{matrix} \\right.\\                                                                                                                                                                                                                                                                          |   |
|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|


$$\sigma( \bullet )$$为狄拉克函数，其性质为：

|   | \\left\\{ \\begin{matrix}                                                                                                                                                 |   |
|   | \\delta\\left( x \\right) = 0,\\forall x \\neq 0 \\\\                                                                                                                     |   |
|   | \\int_{- \\infty}\^{\\infty}{\\delta\\left( x \\right)dx = 1} \\\\                                                                                                        |   |
|   | \\end{matrix} \\right.\\                                                                                                                                                  |   |
|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|


N为数据集$$\mathbb{D}$$的大小。$$p_{\text{model}}(y|\left(
\overrightarrow{x};\overrightarrow{\theta} \right) = \left( y;f\left(
\overrightarrow{x};\overrightarrow{\theta} \right),I \right)$$，则$$J\left(
\overrightarrow{\theta} \right) = -
\mathbb{E}_{\overrightarrow{x},y\sim{\hat{p}}_{\text{data}}}\left| \left| y -
f\left( \overrightarrow{x};\overrightarrow{\theta} \right) \right| \right|^{2} +
const$$，该常量项包含了高斯分布的方差，与$$\overrightarrow{\theta}$$无关。$$\mathbb{E}_{\overrightarrow{x},y\sim{\hat{p}}_{\text{data}}}\log
p_{\text{model}}(y|(\overrightarrow{x};\overrightarrow{\theta})$$其实就是样本的经验分布$${\hat{p}}_{\text{data}}$$与模型$$p_{\text{model}}$$的交叉熵H($${\hat{p}}_{\text{data}},p_{\text{model}}$$)。

1.  激活函数

    1.  线性函数

最简单的是线性函数，它没有非线性。$$\forall\overrightarrow{h}$$，有输出为$$\hat{y}
= W^{T}\overrightarrow{h} +
b$$；若输出层包含多个线性输出单元，则线性输出层为：$$\hat{y} =
W^{T}\overrightarrow{h} + \overrightarrow{b}$$。

它的概率模型为$$p(y|\overrightarrow{x}\mathcal{) =
N}(y;\hat{y},I)$$。高斯分布如下式形式：

|   | f\\left( x \\right) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\^{( - \\frac{{(x - \\mu)}\^{2}}{2\\sigma\^{2}})} |   |
|---|-------------------------------------------------------------------------------------------------------------|---|


给定$$\overrightarrow{x}$$的条件下，y的分布为均值为$$\hat{y}$$，方差为1的高斯分布。求取(2.2.1):$$max(J\left(
\overrightarrow{\theta}
\right))$$就相当于$$min(\mathbb{E}_{\overrightarrow{x},y\sim{\hat{p}}_{\text{data}}}\left|
\left| y - f\left( \overrightarrow{x};\overrightarrow{\theta} \right) \right|
\right|^{2})$$。

### sigmoid函数

sigmoid\~$$P^{x}{(P)}^{1 -
x}$$。二类分类问题可以用伯努利分布来描述。由前面的式子知道：该分布只有一个参数，因此网络仅需求得$$p(y
= 1|\overrightarrow{x})$$。

一种方案是采用线性单元，但是通过阈值来使它位于 [0,1]之间：

|   | p\\left( y = 1 \\middle\| \\overrightarrow{x} \\right) = max\\{ 0,min\\{ 1,W\^{T}\\overrightarrow{h} + b\\}\\} |   |
|---|----------------------------------------------------------------------------------------------------------------|---|


令$$z = W^{T}\overrightarrow{h} + b$$，则上式右侧的函数就是max{0,
min{1,z}}。函数图形如下：

![](media/a5eb45fb5cf50c61989ea4824df10a90.png)

图2.3 max{0, min{1,z}}函数

该函数存在一个问题：当$$p(y = 1|\overrightarrow{x})$$位于[0,1]之外时，$$p(y =
1|\overrightarrow{x})$$对所有的$$z$$求得梯度$$G_{x}$$=0。根据反向传播算法，此时$$p(y
=
1|\overrightarrow{x})$$对于参数$$W^{T}$$和参数b的梯度都为零。从而使得梯度下降算法难以推进。

另一种方案就是利用sigmoid：$$p\left( y = 1 \middle| \overrightarrow{x} \right) =
\sigma(W^{T}\overrightarrow{h} + b)$$，其中$$\sigma( \bullet
)$$表示sigmoid。图形如下：

![](media/37312112522b61927be7a55cc34689d1.png)

图2.4 sigmoid函数

但要注意， sigmoid
函数仅有一段近似线性，但是线性又缺乏网络表达能力，两端平缓区域都不能作为训练使用。

根据

|   | p\\left( y = 1 \\middle\| \\overrightarrow{x} \\right) = \\sigma\\left( z \\right) = \\frac{exp(z)}{exp(z) + exp(0)}     |   |
|---|--------------------------------------------------------------------------------------------------------------------------|---|
|   | p\\left( y = 1 \\middle\| \\overrightarrow{x} \\right) = 1 - \\sigma\\left( z \\right) = \\frac{exp(z)}{exp(z) + exp(0)} |   |

则有

|   | p\\left( y \\middle\| \\overrightarrow{x} \\right) = \\frac{exp(z)}{\\sum_{y\^{'}}\^{}{exp(y\^{'}z)}},y \\in \\{ 0,1\\} |   |
|---|-------------------------------------------------------------------------------------------------------------------------|---|


sigmoid单元的损失函数通常采用负的对数似然函数：

|   | J\\left( \\overrightarrow{\\theta} \\right) = - logp\\left( y \\middle\| \\overrightarrow{x} \\right) = - log\\sigma\\left( \\left( 2y - 1 \\right)z \\right) = \\zeta\\left( \\left( 1 - 2y \\right)z \\right),y \\in \\{ 0,1\\} |   |
|---|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|


其中$$\zeta\left( x \right) = log(1 + exp(x))$$，它是函数$$x^{+} =
max(0,x)$$的一个近似。

![](media/70988fe55f50b78db1960d0ddddd082d.png)

图2.5 softplus函数

由图1.6可知，$$\left( 1 - 2y \right)z \rightarrow - \infty$$的时候，$$J\left(
\overrightarrow{\theta} \right) \rightarrow 0$$。因此有：

-   $$y = 1,\ z \gg 0,1$$

-   $$y = 1,\ z \ll 0,0$$

    1.  softmax函数

softmax：

|   | \\overrightarrow{z} = W\^{T}\\overrightarrow{h} + \\overrightarrow{b}                                                                                                                                                                                       |   |
|---|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|
|   | {\\hat{y}}_{i} = P\\left( y = i \\middle\| \\overrightarrow{x} \\right) = {softmax(\\overrightarrow{z})}_{i} = \\frac{exp(z)}{\\sum_{y\^{'}}\^{}{exp(y\^{'}z)}},i = 1,2,\\ldots,k,\\hat{y} = {({\\hat{y}}_{1},\\ldots,{\\hat{y}}_{k})}\^{T},{\\hat{y}}_{i}i |   |

若变量$$\overrightarrow{z}$$添加一个Constant，形式为：$$\overrightarrow{z} +
C$$，softmax仍然保持原样，即：$$\text{softmax}\left( \overrightarrow{z} \right)
= softmax(\overrightarrow{z} +
C)$$。正是由于此种特性，可以得出另一种形式的softmax：

|   | \\text{softmax}\\left( \\overrightarrow{z} \\right) = softmax(\\overrightarrow{z} + \\operatorname{}z_{i}) |   |
|---|------------------------------------------------------------------------------------------------------------|---|


softmax在其自变量的定义范围内是连续且可微的。当某一$$z_{i} =
\operatorname{}z_{i}$$且有$$z_{i}\  \gg z_{j},\ j = 0,1,\ldots,n,\ i \neq
j$$时，得到的$${\hat{y}}_{i}1$$；$$\forall j = 0,1,\ldots,n,\ i \neq
j,{\hat{y}}_{j}0$$。

假设真实类别为k，则对softmax求解对数：

|   | \\log\\left( \\text{softmax}\\left( \\overrightarrow{z} \\right)_{k} \\right) = z_{k} - log(\\sum_{j}\^{}{exp(z_{j})}) |   |
|---|------------------------------------------------------------------------------------------------------------------------|---|


其中$$z_{k}$$的梯度Grad($$z_{k}$$)$$\neq 0$$，$$log(\sum_{j}^{}{exp(z_{j})})
\approx \operatorname{}z_{i}$$，$$\forall\overrightarrow{z},\max\left(
\log\left( \text{softmax}\left( \overrightarrow{z} \right)_{k} \right)
\right),z_{k}$$项越大越好，$$log(\sum_{j}^{}{exp(z_{j})})$$项越小越好。此时意味着：若真实类别为k，则$$z_{k}$$较大，其它的$$z_{k}$$较小。

代价函数为：$$J\left( z_{k} \right) =$$-$$\log\left( \text{softmax}\left(
\overrightarrow{z} \right)_{k} \right) = \ log(\sum_{j}^{}{\exp\left( z_{j}
\right)} -
z_{k}$$。因此代价函数惩罚那个最活跃的预测（最大的$$z_{j}$$）。如果$$z_{k} =
\max_{j}(z_{j})$$，则代价函数近似为零。若$${\forall j =
0,1,\ldots,n,,z}_{j}0z_{j} < 0,$$则$$\text{softmax}\left( \overrightarrow{z}
\right)_{k}0$$，则有$$\log\left( \text{softmax}\left( \overrightarrow{z}
\right)_{k} \right) - \infty$$，这样是不稳定的。

### 修正线性函数

修正线性单元的激活函数：

|   | g\\left( z \\right) = max\\{ 0,z\\} |   |
|---|-------------------------------------|---|


图像如图2.6所示：

![](media/0bbd97635bc48e5f9c65ea61a2521225.png)

图2.6 线性修正函数图

由式(2.3.11)可以看出，当$$z > 0$$的时候它是线性的，当$$z <
0$$的时候，它是横坐标的左半边坐标轴。

-   优点：使用与梯度相关的优化算法这种函数非常易于优化。当该函数$$z >
    0$$(激活)工作的时候，求导得常量1；当该函数$$z <
    0$$(非激活)工作的时候，求导得常量$$\theta$$。该函数对任一自变量取值，二阶求导皆为常量0。

-   缺点：当该函数$$z <
    0$$(非激活)工作的时候，很难求解函数参数，因为此时梯度为零。

对于修正性单元$$\overrightarrow{h} = {g(W}^{T}\overrightarrow{x} +
\overrightarrow{b})$$，根据式(2.3.11)，尽量使$$z >
0$$；所以我们设置$$\overrightarrow{b} = C,\ C > 0C \ll
1$$。这样一来修正函数会处于坐标的正半轴部分使得整个函数处于工作状态。

修正函数有三个变体形式可是使其更好的工作：

当$$z < 0$$的时候，添加斜率$$\propto ,\  \propto \neq 0$$；有：

|   | h = g\\left( z, \\propto \\right) = \\max\\left( 0,z \\right) + \\propto min(0,z) |   |
|---|-----------------------------------------------------------------------------------|---|


-   使$$\propto = - 1,g\left( z \right) = \left| z \right|$$。如图2.2所示。

-   使$$\propto > 0 \propto \ll 1$$。如图2.3所示。

-   在网络训练中学习$$\propto$$，不同的模型学习的$$\propto$$有所差异。
